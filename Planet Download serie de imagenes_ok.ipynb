{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Este script baja imagen segun un aoi dibujado interactivamente, y luego, clipea con el aoi (sobre la imagen ya bajada)\n",
    "* busca imagenes por filtros\n",
    "* genera un calculo de overlapping con el aoi determinado\n",
    "* filtra de la base total aquellas imagenes que cumplan con un criterio (por ej, :x% overlaping)\n",
    "* activa el asset que se quiera, y baja las imagenes zipeadas en un directorio a definir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import scipy\n",
    "import urllib\n",
    "import datetime \n",
    "import urllib3\n",
    "import rasterio\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from osgeo import gdal\n",
    "from planet import api\n",
    "from planet.api import filters\n",
    "from traitlets import link\n",
    "import rasterio.mask as rio_mask\n",
    "from shapely.geometry import mapping, shape\n",
    "from IPython.display import display, Image, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "urllib3.disable_warnings()\n",
    "from ipyleaflet import (\n",
    "    Map,\n",
    "    Marker,\n",
    "    TileLayer, ImageOverlay,\n",
    "    Polyline, Polygon, Rectangle, Circle, CircleMarker,\n",
    "    GeoJSON,\n",
    "    DrawControl\n",
    ")\n",
    "\n",
    "api_keys = json.load(open(\"apikeys.json\",'r'))\n",
    "client = api.ClientV1(api_key=api_keys[\"PLANET_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a slippy map to get GeoJSON\n",
    "The planet API allows you to query using a geojson which is a special flavor of json.\n",
    "We are going to create a slippy map using leaflet and apply the Planet 2017 Q1 mosaic as the basemap. This requires our api key.\n",
    "We are going to add a special draw handler that shoves a draw region into a object so we get the geojson.\n",
    "If you don't want to do this, or need a fixed query try geojson.io\n",
    "To install and run:\n",
    "$ pip install ipyleaflet\n",
    "$ jupyter nbextension enable --py --sys-prefix ipyleaflet\n",
    "$ jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "More information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filtros y definir aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myAOI = json.load(open(\"estanzuela.json\",'r'))\n",
    "#print(myAOI)\n",
    "# build a query using the AOI and\n",
    "# a cloud_cover filter that excludes 'cloud free' scenes\n",
    "\n",
    "old = datetime.datetime(year=2017,month=10,day=1)\n",
    "\n",
    "query = filters.and_filter(\n",
    "    filters.geom_filter(myAOI),\n",
    "    filters.range_filter('cloud_cover', lt=10),\n",
    "    filters.date_range('acquired', gt=old)\n",
    ")\n",
    "\n",
    "# build a request for only PlanetScope imagery\n",
    "request = filters.build_search_request(\n",
    "    query, item_types=['PSScene4Band']\n",
    ")\n",
    "\n",
    "# if you don't have an API key configured, this will raise an exception\n",
    "result = client.quick_search(request)\n",
    "#print(result.items_iter)\n",
    "scenes = []\n",
    "planet_map = {}\n",
    "for item in result.items_iter(limit=500):\n",
    "    planet_map[item['id']]=item\n",
    "    props = item['properties']\n",
    "    props[\"id\"] = item[\"id\"]\n",
    "    props[\"geometry\"] = item[\"geometry\"]\n",
    "    props[\"thumbnail\"] = item[\"_links\"][\"thumbnail\"]\n",
    "    #print(props)\n",
    "    scenes.append(props)\n",
    "    #print(scenes)\n",
    "scenes = pd.DataFrame(data=scenes)\n",
    "display(scenes)\n",
    "print(len(scenes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "The data we got back is good, but we need some more information\n",
    "We got back big scenes, but we only care about our area of interest. The scene may not cover the whole area of interest.\n",
    "We can use the Shapely library to quickly figure out how much each scene overlaps our AOI\n",
    "We will convert our AOI and the geometry of each scene to calculate overlap using a shapely call.\n",
    "The returned acquisition, publish, and update times are strings, we'll convert them to datatime objects so we wan search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's clean up the datetime stuff\n",
    "# make a shapely shape from our aoi\n",
    "AOI = shape(myAOI)\n",
    "footprints = []\n",
    "overlaps = []\n",
    "# go through the geometry from our api call, convert to a shape and calculate overlap area.\n",
    "# also save the shape for safe keeping\n",
    "for footprint in scenes[\"geometry\"].tolist():\n",
    "    s = shape(footprint)\n",
    "    footprints.append(s)\n",
    "    overlap = 100.0*(AOI.intersection(s).area / AOI.area)\n",
    "    overlaps.append(overlap)\n",
    "# take our lists and add them back to our dataframe\n",
    "scenes['overlap'] = pd.Series(overlaps, index=scenes.index)\n",
    "scenes['footprint'] = pd.Series(footprints, index=scenes.index)\n",
    "# now make sure pandas knows about our date/time columns.\n",
    "scenes[\"acquired\"] = pd.to_datetime(scenes[\"acquired\"])\n",
    "scenes[\"published\"] = pd.to_datetime(scenes[\"published\"])\n",
    "scenes[\"updated\"] = pd.to_datetime(scenes[\"updated\"])\n",
    "scenes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering our search using pandas.\n",
    "* Using our dataframe we will filter the scenes to just what we want.\n",
    "* First we want scenes with less than 10% clouds.\n",
    "* Second we want standard quality images. Test images may not be high quality.\n",
    "* Third well only look for scenes since January.\n",
    "* Finally we will create a new data frame with our queries and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's get it down to just good, recent, clear scenes\n",
    "clear = scenes['cloud_cover']<0.1\n",
    "good = scenes['quality_category']==\"standard\"\n",
    "recent = scenes[\"acquired\"] > datetime.date(year=2018,month=1,day=1)\n",
    "partial_coverage = scenes[\"overlap\"] >90\n",
    "good_scenes = scenes[(good&clear&recent&partial_coverage)]\n",
    "display(good_scenes)\n",
    "print (len(good_scenes))\n",
    "\n",
    "# Now let's get it down to just good, recent, clear scenes\n",
    "clear = scenes['cloud_cover']<0.5\n",
    "good = scenes['quality_category']==\"standard\"\n",
    "all_time = scenes[\"acquired\"] > datetime.date(year=2017,month=11,day=1)\n",
    "full_coverage = scenes[\"overlap\"] >= 60\n",
    "all_scenes = scenes[(good&clear&all_time&full_coverage)]\n",
    "#display(all_scenes)\n",
    "print (len(all_scenes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Activation and Downloading\n",
    "There are two things we need to know, the satellite type (asset) and image type (product).\n",
    "Full resolution uncompressed satellite images are big and there are lots of ways to view them.\n",
    "For this reason Planet generally keeps images in their native format and only processes them on customer requests. There is some caching of processed scenes, but this is the exception not the rule.\n",
    "All images must be activated prior to downloading and this can take some time based on demand.\n",
    "Additionally we need to determine what sort of product we want to download. Generally speaking there are three kinds of scenes:\n",
    "Analytic - multi-band full resolution images that have not been processed. These are like raw files for DSLR camers.\n",
    "Visual - these are color corrected rectified tifs. If you are just starting out this is your best call.\n",
    "UDM - Usable data mask. This mask can be used to find bad pixels and columns and to mask out areas with clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good_scenes=pd.Series.tolist(good_scenes)\n",
    "print(good_scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Set Item Type\n",
    "item_type = 'PSScene4Band'\n",
    "\n",
    "# Set Asset Type\n",
    "asset_type = 'analytic_sr'\n",
    "\n",
    "#lista de los nombres de imagenes a bajar (era scene_id)\n",
    "to_get=all_scenes['id'].tolist()\n",
    "print(to_get)\n",
    "#calcular el area de la sumatoria de las imagenes#\n",
    "#\n",
    "api_key=api_keys[\"PLANET_API_KEY\"]\n",
    "url_base=[]\n",
    "i=0\n",
    "for scenes in to_get[0:3]:\n",
    "# Request clip of scene (This will take some time to complete)\n",
    "    #calcular el area de la seleccion de imagenes hecha e imprimir#\n",
    "    \n",
    "    clip_payload = {'aoi': myAOI,'targets': [{'item_id': to_get[i],'item_type': item_type,'asset_type': asset_type}]}\n",
    "    #print(clip_payload)\n",
    "    i=i+1\n",
    "    request = requests.post('https://api.planet.com/compute/ops/clips/v1', auth=(api_key, ''), json=clip_payload)\n",
    "    print(request)\n",
    "    clip_url= request.json()['_links']['_self']\n",
    "    print(clip_url)\n",
    "    # Poll API to monitor clip status. Once finished, download and upzip the scene\n",
    "    clip_succeeded = False\n",
    "    while not clip_succeeded:\n",
    "\n",
    "    # Poll API\n",
    "        check_state_request = requests.get(clip_url, auth=(api_key, ''))\n",
    "        #print(check_state_request)\n",
    "    # If clipping process succeeded , we are done\n",
    "        if check_state_request.json()['state'] == 'succeeded':\n",
    "            clip_download_url = check_state_request.json()['_links']['results'][0]\n",
    "            clip_succeeded = True\n",
    "            #print(clip_download_url)\n",
    "            if clip_download_url in url_base: continue\n",
    "            url_base.append(clip_download_url)\n",
    "            print(\"Clip of scene succeeded and is ready to download\") \n",
    "    \n",
    "    # Still activating. Wait 1 second and check again.\n",
    "        else:\n",
    "            print(\"...Still waiting for clipping to complete...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "print(len(url_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and upzip the clip\n",
    "Once complete, look in the output directory to see your clipped tif file.\n",
    "\n",
    "NOTE: Clipped scene will only be available for 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "# ubicar los archivos donde se desee\n",
    "path_or='D:/javie/Descargas/'\n",
    "\n",
    "print(to_get[0:3])\n",
    "#print(url_base)\n",
    "i=0    \n",
    "downloaded=[]\n",
    "for url in url_base[0:2]: \n",
    "    response = requests.get(url, stream=True)\n",
    "    #print(response)\n",
    "    scene_id=to_get[i]\n",
    "    print(scene_id)\n",
    "    path =os.path.join(path_or+ scene_id +'_'+ asset_type +'_'+ myAOI['name'])\n",
    "    #print(path)\n",
    "    i= i+1\n",
    "    if os.path.isfile(path + '.zip' ):\n",
    "        print (\"skipping, we allready have scene: \", scene_id)\n",
    "        continue \n",
    "    else:\n",
    "        with open(path +'.zip', \"wb\") as handle:\n",
    "            for data in tqdm(response.iter_content()):\n",
    "                #print(data)\n",
    "                handle.write(data)\n",
    "        downloaded.append(path)\n",
    "        print(downloaded)\n",
    "\n",
    "        date=datetime.now()\n",
    "        date=date.strftime('%Y_%m_%d_%H%M')\n",
    "\n",
    "        newpath=os.path.join(path_or, date + '_downloaded.txt')\n",
    "        #print(newpath)\n",
    "        with open (newpath , \"w\") as pronto:\n",
    "            for item in downloaded:\n",
    "                pronto.write(\"%s\\n\" % item)       \n",
    "    #Unzip file\n",
    "        ziped_item = zipfile.ZipFile(path + \".zip\")\n",
    "        ziped_item.extractall(path)     \n",
    "  \n",
    "    # Delete zip file\n",
    "        #time.sleep(5)\n",
    "        #os.remove(path + '.zip')\n",
    "        print('Downloaded clips located in:', path_or)    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
